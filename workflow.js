// LangGraph Workflow Visualization
const workflowData = {
    nodes: [
        // Start
        { id: 'start', label: 'START', type: 'start', x: 600, y: 30 },
        
        // Indexing nodes
        { id: 'doc_loading', label: 'doc_loading', type: 'indexing', x: 600, y: 120 },
        { id: 'doc_chunking', label: 'doc_chunking', type: 'indexing', x: 600, y: 210 },
        { id: 'doc_embedding', label: 'doc_embedding', type: 'indexing', x: 400, y: 320 },
        { id: 'keyword_extract', label: 'keyword_extract', type: 'indexing', x: 800, y: 320 },
        { id: 'neo4j_save', label: 'neo4j_save', type: 'indexing', x: 400, y: 430 },
        { id: 'fc_extract', label: 'fc_extract', type: 'indexing', x: 800, y: 430 },
        { id: 'fc_csv_save', label: 'fc_csv_save', type: 'indexing', x: 800, y: 540 },
        
        // Router
        { id: 'search_routing', label: 'search_routing', type: 'router', x: 600, y: 650 },
        
        // Search nodes
        { id: 'vector_search', label: 'vector_search', type: 'search', x: 150, y: 760 },
        { id: 'graph_search', label: 'graph_search', type: 'search', x: 375, y: 760 },
        { id: 'lattice_oc_search', label: 'lattice_oc_search', type: 'search', x: 600, y: 760 },
        { id: 'lattice_jaccard_search', label: 'lattice_jaccard_search', type: 'search', x: 825, y: 760 },
        { id: 'hybrid_search', label: 'hybrid_search', type: 'search', x: 1050, y: 760 },
        
        // Quality evaluation
        { id: 'search_quality_eval', label: 'search_quality_eval', type: 'quality', x: 600, y: 870 },
        
        // Generation nodes
        { id: 'context_integration', label: 'context_integration', type: 'generation', x: 600, y: 980 },
        { id: 'prompt_generate', label: 'prompt_generate', type: 'generation', x: 600, y: 1070 },
        { id: 'answer_generate', label: 'answer_generate', type: 'generation', x: 600, y: 1160 },
        { id: 'answer_quality_eval', label: 'answer_quality_eval', type: 'quality', x: 600, y: 1250 },
        { id: 'hallucination_detect', label: 'hallucination_detect', type: 'quality', x: 600, y: 1340 },
        { id: 'query_rewrite', label: 'query_rewrite', type: 'generation', x: 900, y: 980 },
        { id: 'answer_post', label: 'answer_post', type: 'generation', x: 600, y: 1430 },
        
        // End
        { id: 'end', label: 'END', type: 'end', x: 600, y: 1520 }
    ],
    
    links: [
        // Initial flow
        { source: 'start', target: 'doc_loading', type: 'normal' },
        { source: 'start', target: 'search_routing', type: 'conditional', label: 'indexed' },
        
        // Indexing flow
        { source: 'doc_loading', target: 'doc_chunking', type: 'normal' },
        { source: 'doc_chunking', target: 'doc_embedding', type: 'normal' },
        { source: 'doc_chunking', target: 'keyword_extract', type: 'normal' },
        { source: 'doc_embedding', target: 'neo4j_save', type: 'normal' },
        { source: 'keyword_extract', target: 'fc_extract', type: 'normal' },
        { source: 'fc_extract', target: 'fc_csv_save', type: 'normal' },
        { source: 'neo4j_save', target: 'search_routing', type: 'conditional' },
        { source: 'fc_csv_save', target: 'search_routing', type: 'conditional' },
        
        // Search routing
        { source: 'search_routing', target: 'vector_search', type: 'conditional', label: 'simple' },
        { source: 'search_routing', target: 'graph_search', type: 'conditional', label: 'relation' },
        { source: 'search_routing', target: 'lattice_oc_search', type: 'conditional', label: 'hierarchy' },
        { source: 'search_routing', target: 'lattice_jaccard_search', type: 'conditional', label: 'similarity' },
        { source: 'search_routing', target: 'hybrid_search', type: 'conditional', label: 'complex' },
        
        // Quality evaluation
        { source: 'vector_search', target: 'search_quality_eval', type: 'normal' },
        { source: 'graph_search', target: 'search_quality_eval', type: 'normal' },
        { source: 'lattice_oc_search', target: 'search_quality_eval', type: 'normal' },
        { source: 'lattice_jaccard_search', target: 'search_quality_eval', type: 'normal' },
        { source: 'hybrid_search', target: 'search_quality_eval', type: 'normal' },
        
        { source: 'search_quality_eval', target: 'context_integration', type: 'conditional', label: 'pass' },
        { source: 'search_quality_eval', target: 'query_rewrite', type: 'conditional', label: 'fail' },
        
        // Generation flow
        { source: 'context_integration', target: 'prompt_generate', type: 'normal' },
        { source: 'prompt_generate', target: 'answer_generate', type: 'normal' },
        { source: 'answer_generate', target: 'answer_quality_eval', type: 'normal' },
        { source: 'answer_quality_eval', target: 'hallucination_detect', type: 'normal' },
        
        { source: 'hallucination_detect', target: 'answer_post', type: 'conditional', label: 'pass' },
        { source: 'hallucination_detect', target: 'query_rewrite', type: 'conditional', label: 'rewrite' },
        { source: 'hallucination_detect', target: 'search_routing', type: 'conditional', label: 'research' },
        
        { source: 'query_rewrite', target: 'search_routing', type: 'conditional' },
        { source: 'query_rewrite', target: 'answer_post', type: 'conditional', label: 'max_iter' },
        
        { source: 'answer_post', target: 'end', type: 'normal' }
    ]
};

// Color scheme for node types
const nodeColors = {
    start: '#10b981',
    end: '#ef4444',
    indexing: '#3b82f6',
    router: '#8b5cf6',
    search: '#f59e0b',
    quality: '#ec4899',
    generation: '#06b6d4'
};

// Initialize D3 visualization
function initializeWorkflow() {
    const svg = d3.select('#workflow-svg');
    const width = 1200;
    const height = 1600;
    
    svg.attr('viewBox', `0 0 ${width} ${height}`)
       .attr('preserveAspectRatio', 'xMidYMid meet');
    
    // Define arrow marker
    svg.append('defs').append('marker')
        .attr('id', 'arrowhead')
        .attr('markerWidth', 10)
        .attr('markerHeight', 10)
        .attr('refX', 9)
        .attr('refY', 3)
        .attr('orient', 'auto')
        .append('polygon')
        .attr('points', '0 0, 10 3, 0 6')
        .style('fill', '#6b7280');
    
    // Create links
    const links = svg.append('g')
        .attr('class', 'links')
        .selectAll('g')
        .data(workflowData.links)
        .enter()
        .append('g');
    
    // Draw curved paths
    links.append('path')
        .attr('class', d => `link link-${d.type}`)
        .attr('id', d => `link-${d.source}-${d.target}`)
        .attr('d', d => {
            const source = workflowData.nodes.find(n => n.id === d.source);
            const target = workflowData.nodes.find(n => n.id === d.target);
            
            const dx = target.x - source.x;
            const dy = target.y - source.y;
            const dr = Math.sqrt(dx * dx + dy * dy) * 0.5;
            
            // Straight line for normal links, curved for conditional
            if (d.type === 'normal') {
                return `M${source.x},${source.y + 25} L${target.x},${target.y - 25}`;
            } else {
                return `M${source.x},${source.y + 25} Q${(source.x + target.x) / 2 + dx * 0.2},${(source.y + target.y) / 2} ${target.x},${target.y - 25}`;
            }
        });
    
    // Add link labels
    links.filter(d => d.label)
        .append('text')
        .attr('class', 'link-label')
        .attr('dy', -5)
        .append('textPath')
        .attr('href', d => `#link-${d.source}-${d.target}`)
        .attr('startOffset', '50%')
        .style('text-anchor', 'middle')
        .style('font-size', '10px')
        .style('fill', '#6b7280')
        .text(d => d.label);
    
    // Create nodes
    const nodes = svg.append('g')
        .attr('class', 'nodes')
        .selectAll('g')
        .data(workflowData.nodes)
        .enter()
        .append('g')
        .attr('class', 'node')
        .attr('id', d => `node-${d.id}`)
        .attr('transform', d => `translate(${d.x}, ${d.y})`)
        .on('click', showNodeDetails)
        .on('mouseover', highlightConnections)
        .on('mouseout', resetHighlight);
    
    // Node rectangles
    nodes.append('rect')
        .attr('x', d => -d.label.length * 6 - 15)
        .attr('y', -25)
        .attr('width', d => d.label.length * 12 + 30)
        .attr('height', 50)
        .attr('rx', 8)
        .attr('ry', 8)
        .style('fill', d => nodeColors[d.type])
        .style('stroke', '#374151')
        .style('stroke-width', 2);
    
    // Node labels
    nodes.append('text')
        .attr('text-anchor', 'middle')
        .attr('dy', 5)
        .style('fill', 'white')
        .style('font-weight', '600')
        .style('font-size', '14px')
        .text(d => d.label);
}

// Highlight path functions
function highlightPath(pathType) {
    // Reset all highlights first
    d3.selectAll('.link').classed('highlighted', false);
    d3.selectAll('.node rect').style('opacity', 0.3);
    
    let nodesToHighlight = [];
    
    switch(pathType) {
        case 'indexing':
            nodesToHighlight = ['start', 'doc_loading', 'doc_chunking', 'doc_embedding', 
                              'keyword_extract', 'neo4j_save', 'fc_extract', 'fc_csv_save'];
            break;
        case 'vector':
            nodesToHighlight = ['search_routing', 'vector_search', 'search_quality_eval', 
                              'context_integration', 'prompt_generate', 'answer_generate', 
                              'answer_quality_eval', 'hallucination_detect', 'answer_post', 'end'];
            break;
        case 'graph':
            nodesToHighlight = ['search_routing', 'graph_search', 'search_quality_eval', 
                              'context_integration', 'prompt_generate', 'answer_generate', 
                              'answer_quality_eval', 'hallucination_detect', 'answer_post', 'end'];
            break;
        case 'lattice':
            nodesToHighlight = ['search_routing', 'lattice_oc_search', 'search_quality_eval', 
                              'context_integration', 'prompt_generate', 'answer_generate', 
                              'answer_quality_eval', 'hallucination_detect', 'answer_post', 'end'];
            break;
    }
    
    // Highlight nodes
    nodesToHighlight.forEach(nodeId => {
        d3.select(`#node-${nodeId} rect`).style('opacity', 1);
    });
    
    // Highlight links between highlighted nodes
    workflowData.links.forEach(link => {
        if (nodesToHighlight.includes(link.source) && nodesToHighlight.includes(link.target)) {
            d3.select(`#link-${link.source}-${link.target}`).classed('highlighted', true);
        }
    });
}

function resetHighlight() {
    d3.selectAll('.link').classed('highlighted', false);
    d3.selectAll('.node rect').style('opacity', 1);
}

function highlightConnections(event, d) {
    // Highlight connected nodes and edges on hover
    const connectedNodes = new Set([d.id]);
    
    workflowData.links.forEach(link => {
        if (link.source === d.id) connectedNodes.add(link.target);
        if (link.target === d.id) connectedNodes.add(link.source);
    });
    
    d3.selectAll('.node rect').style('opacity', n => connectedNodes.has(n.id) ? 1 : 0.3);
    d3.selectAll('.link').style('opacity', l => 
        (l.source === d.id || l.target === d.id) ? 1 : 0.2
    );
}

// Node detail information
const nodeDetails = {
    'start': {
        title: 'START - ì‹œì‘ ë…¸ë“œ',
        category: 'system',
        description: 'ì›Œí¬í”Œë¡œìš°ì˜ ì‹œì‘ì ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.',
        function: 'ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ë¼ìš°íŒ… ê²°ì •',
        details: `â€¢ ì¸ë±ì‹± ì™„ë£Œ ì—¬ë¶€ í™•ì¸
â€¢ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ê²€ìƒ‰ ëª¨ë“œë¡œ ì§„ì…
â€¢ ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ì¸ë±ì‹± ëª¨ë“œë¡œ ì§„ì…`,
        input: 'ì‚¬ìš©ì ì§ˆë¬¸ (ì„ íƒì )',
        output: 'ì´ˆê¸° ìƒíƒœ ì„¤ì •',
        code: `def should_start_indexing(state):
    question = state.get("question", "").strip()
    if not question:
        return "doc_loading"  # ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ì¸ë±ì‹±
    
    if state.get("indexing_complete", False):
        return "search_routing"  # ì¸ë±ì‹± ì™„ë£Œë©´ ê²€ìƒ‰
    return "doc_loading"  # ì¸ë±ì‹± í•„ìš”`
    },
    
    'doc_loading': {
        title: 'doc_loading - ë¬¸ì„œ ë¡œë”©',
        category: 'indexing',
        description: 'JSON ë¬¸ì„œë¥¼ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ë¡œë“œí•˜ê³  íŒŒì‹±í•©ë‹ˆë‹¤.',
        function: 'ë°ì´í„° íŒŒì¼ ì½ê¸° ë° JSON íŒŒì‹±',
        details: `â€¢ data/ í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ ë¡œë“œ
â€¢ ê° íŒŒì¼ì„ Python ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
â€¢ ì—ëŸ¬ ì²˜ë¦¬ ë° ê²€ì¦ ìˆ˜í–‰`,
        input: 'data/technical_docs.json',
        output: 'List[Dict] - íŒŒì‹±ëœ ë¬¸ì„œë“¤',
        code: `def doc_loading(state):
    json_data = []
    for filename in os.listdir("data"):
        if filename.endswith(".json"):
            with open(f"data/{filename}", 'r') as f:
                json_data.append(json.load(f))
    return {"documents": json_data}`
    },
    
    'doc_chunking': {
        title: 'doc_chunking - ë¬¸ì„œ ì²­í‚¹',
        category: 'indexing',
        description: 'ë¬¸ì„œë¥¼ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.',
        function: 'í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰ ê°€ëŠ¥í•œ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• ',
        details: `â€¢ ì„¹ì…˜ë³„ ë¶„í•  (í—¤ë” ê¸°ì¤€)
â€¢ ê³ ì • í¬ê¸° ë¶„í•  (í† í° ìˆ˜ ê¸°ì¤€)
â€¢ ì˜¤ë²„ë© ì„¤ì •ìœ¼ë¡œ ë¬¸ë§¥ ë³´ì¡´
â€¢ ë©”íƒ€ë°ì´í„° ìœ ì§€`,
        input: 'documents: List[Dict]',
        output: 'chunks: List[Chunk]',
        code: `def doc_chunking(state):
    chunks = []
    for doc in state["documents"]:
        sections = split_by_sections(doc)
        for section in sections:
            chunk = {
                "text": section["content"],
                "metadata": {
                    "source": doc["title"],
                    "section": section["header"]
                }
            }
            chunks.append(chunk)
    return {"chunks": chunks}`
    },
    
    'doc_embedding': {
        title: 'doc_embedding - ì„ë² ë”© ìƒì„±',
        category: 'indexing',
        description: 'í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.',
        function: 'OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ìƒì„±',
        details: `â€¢ OpenAI text-embedding-ada-002 ì‚¬ìš©
â€¢ 1536 ì°¨ì› ë²¡í„° ìƒì„±
â€¢ ë°°ì¹˜ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± í–¥ìƒ
â€¢ ë¹„ìš© ìµœì í™”`,
        input: 'chunks: List[Chunk]',
        output: 'embeddings: List[Vector]',
        code: `def doc_embedding(state):
    embedder = OpenAIEmbeddings(
        model="text-embedding-ada-002"
    )
    texts = [chunk["text"] for chunk in state["chunks"]]
    embeddings = embedder.embed_documents(texts)
    
    for chunk, embedding in zip(state["chunks"], embeddings):
        chunk["embedding"] = embedding
    
    return {"embedding_complete": True}`
    },
    
    'keyword_extract': {
        title: 'keyword_extract - í‚¤ì›Œë“œ ì¶”ì¶œ',
        category: 'indexing',
        description: 'spaCyë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.',
        function: 'NER ë° TF-IDF ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ',
        details: `â€¢ Named Entity Recognition (NER)
â€¢ TF-IDF ì ìˆ˜ ê³„ì‚°
â€¢ ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ ì‚¬ì „ í™œìš©
â€¢ ë¶ˆìš©ì–´ ì œê±° ë° ì •ê·œí™”`,
        input: 'chunks: List[Chunk]',
        output: 'keywords: List[str]',
        code: `def keyword_extract(state):
    nlp = spacy.load("en_core_web_sm")
    keywords = []
    
    for chunk in state["chunks"]:
        doc = nlp(chunk["text"])
        # NER
        entities = [ent.text for ent in doc.ents]
        # ëª…ì‚¬êµ¬ ì¶”ì¶œ
        noun_phrases = [chunk.text for chunk in doc.noun_chunks]
        keywords.extend(entities + noun_phrases)
    
    return {"keywords": list(set(keywords))}`
    },
    
    'neo4j_save': {
        title: 'neo4j_save - ê·¸ë˜í”„ DB ì €ì¥',
        category: 'indexing',
        description: 'Neo4j ê·¸ë˜í”„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.',
        function: 'ë…¸ë“œì™€ ê´€ê³„ ìƒì„±',
        details: `â€¢ Chunk ë…¸ë“œ ìƒì„±
â€¢ ì„ë² ë”© í”„ë¡œí¼í‹° ì¶”ê°€
â€¢ ë¬¸ì„œ ê°„ ê´€ê³„ ì„¤ì •
â€¢ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±`,
        input: 'chunks with embeddings',
        output: 'graph_complete: True',
        code: `def neo4j_save(state):
    for chunk in state["chunks"]:
        query = """
        CREATE (c:Chunk {
            text: $text,
            embedding: $embedding,
            source: $source
        })
        """
        graph.query(query, {
            "text": chunk["text"],
            "embedding": chunk["embedding"],
            "source": chunk["metadata"]["source"]
        })
    return {"graph_complete": True}`
    },
    
    'fc_extract': {
        title: 'fc_extract - FCA ë¶„ì„',
        category: 'indexing',
        description: 'Formal Concept Analysisë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.',
        function: 'ê°œë… ê²©ì êµ¬ì¶•',
        details: `â€¢ ë¬¸ì„œ-í‚¤ì›Œë“œ ì´ì§„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
â€¢ í˜•ì‹ ê°œë… ì¶”ì¶œ
â€¢ ê°œë… ê°„ ê³„ì¸µ ê´€ê³„ ê³„ì‚°
â€¢ ê²©ì êµ¬ì¡° ìƒì„±`,
        input: 'documents, keywords',
        output: 'concept_lattice',
        code: `def fc_extract(state):
    # ì´ì§„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    binary_matrix = create_binary_matrix(
        state["documents"], 
        state["keywords"]
    )
    
    # FCA ìˆ˜í–‰
    context = Context(binary_matrix)
    concepts = context.intension()
    lattice = build_lattice(concepts)
    
    return {"concept_lattice": lattice}`
    },
    
    'fc_csv_save': {
        title: 'fc_csv_save - FCA ì €ì¥',
        category: 'indexing',
        description: 'FCA ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.',
        function: 'CSV í˜•ì‹ìœ¼ë¡œ ê°œë… ê²©ì ì €ì¥',
        details: `â€¢ formal_concept.csv ìƒì„±
â€¢ lattice_binary_table.csv ìƒì„±
â€¢ ê³„ì¸µ ê´€ê³„ ì •ë³´ í¬í•¨
â€¢ ì¶”í›„ ë¹ ë¥¸ ë¡œë“œë¥¼ ìœ„í•œ ìºì‹±`,
        input: 'concept_lattice',
        output: 'fc_complete: True',
        code: `def fc_csv_save(state):
    lattice_df = pd.DataFrame(state["concept_lattice"])
    lattice_df.to_csv("db/formal_concept.csv", index=False)
    
    binary_df = pd.DataFrame(state["binary_matrix"])
    binary_df.to_csv("db/lattice_binary_table.csv")
    
    return {"fc_complete": True}`
    },
    
    'search_routing': {
        title: 'search_routing - ê²€ìƒ‰ ë¼ìš°íŒ…',
        category: 'router',
        description: 'ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ê²€ìƒ‰ ë°©ë²•ì„ ê²°ì •í•©ë‹ˆë‹¤.',
        function: 'ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ë° ê²€ìƒ‰ ì „ëµ ì„ íƒ',
        details: `â€¢ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
â€¢ LLM ê¸°ë°˜ ì˜ë„ íŒŒì•… (ì„ íƒì )
â€¢ 5ê°€ì§€ ê²€ìƒ‰ ì „ëµ ì¤‘ ì„ íƒ
â€¢ ì¬ê²€ìƒ‰ íšŸìˆ˜ ê´€ë¦¬`,
        input: 'question: str',
        output: 'search_method: str',
        code: `def search_routing(state):
    question = state["question"].lower()
    
    if "ê³„ì¸µ" in question or "ìƒìœ„" in question:
        return "hierarchy_query"
    elif "ìœ ì‚¬" in question or "ë¹„ìŠ·" in question:
        return "similarity_concept_query"
    elif "ê´€ê³„" in question or "ì—°ê²°" in question:
        return "relationship_query"
    elif "ì „ì²´" in question or "ì¢…í•©" in question:
        return "complex_query"
    else:
        return "simple_query"`
    },
    
    'vector_search': {
        title: 'vector_search - ë²¡í„° ê²€ìƒ‰',
        category: 'search',
        description: 'ì„ë² ë”© ë²¡í„°ë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.',
        function: 'ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰',
        details: `â€¢ ì§ˆë¬¸ ì„ë² ë”© ìƒì„±
â€¢ Neo4j ë²¡í„° ì¸ë±ìŠ¤ ê²€ìƒ‰
â€¢ Top-K ê²°ê³¼ ì¶”ì¶œ
â€¢ ìœ ì‚¬ë„ ì ìˆ˜ ê¸°ë°˜ í•„í„°ë§`,
        input: 'question embedding',
        output: 'search_results: List[Chunk]',
        code: `def vector_search(state):
    query_embedding = embedder.embed(state["question"])
    
    cypher = """
    MATCH (c:Chunk)
    WITH c, gds.similarity.cosine(
        c.embedding, $embedding
    ) AS score
    WHERE score > $threshold
    RETURN c.text, score
    ORDER BY score DESC
    LIMIT $top_k
    """
    
    results = graph.query(cypher, {
        "embedding": query_embedding,
        "threshold": 0.7,
        "top_k": 3
    })
    return {"search_results": results}`
    },
    
    'graph_search': {
        title: 'graph_search - ê·¸ë˜í”„ ê²€ìƒ‰',
        category: 'search',
        description: 'Neo4jì—ì„œ ê´€ê³„ ê¸°ë°˜ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.',
        function: 'ì—”í‹°í‹° ê°„ ê´€ê³„ íƒìƒ‰',
        details: `â€¢ ì—”í‹°í‹° ì¶”ì¶œ
â€¢ ê´€ê³„ íŒ¨í„´ ë§¤ì¹­
â€¢ ê²½ë¡œ íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜
â€¢ ê´€ê³„ ê°€ì¤‘ì¹˜ ê³ ë ¤`,
        input: 'entities: List[str]',
        output: 'related_chunks: List[Chunk]',
        code: `def graph_search(state):
    entities = extract_entities(state["question"])
    
    cypher = """
    MATCH (n1:Entity)-[r]-(n2:Entity)
    WHERE n1.name IN $entities
    OPTIONAL MATCH (n1)-[*1..2]-(related:Chunk)
    RETURN n1, r, n2, collect(related) as chunks
    ORDER BY r.weight DESC
    """
    
    results = graph.query(cypher, {"entities": entities})
    return {"search_results": results}`
    },
    
    'lattice_oc_search': {
        title: 'lattice_oc_search - ê³„ì¸µ ê²€ìƒ‰',
        category: 'search',
        description: 'FCA ê²©ìì—ì„œ ê³„ì¸µì  ê´€ê³„ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.',
        function: 'Order Coverage ì•Œê³ ë¦¬ì¦˜',
        details: `â€¢ ê°œë… ì¶”ì¶œ
â€¢ ìƒìœ„ ê°œë… (ì¼ë°˜í™”) íƒìƒ‰
â€¢ í•˜ìœ„ ê°œë… (íŠ¹ìˆ˜í™”) íƒìƒ‰
â€¢ ê³„ì¸µ ë ˆë²¨ ê³„ì‚°`,
        input: 'concept: str',
        output: 'hierarchy: Dict',
        code: `def lattice_oc_search(state):
    concept = extract_concept(state["question"])
    lattice = load_lattice()
    
    # ìƒìœ„ ê°œë…
    super_concepts = lattice.get_super_concepts(concept)
    # í•˜ìœ„ ê°œë…
    sub_concepts = lattice.get_sub_concepts(concept)
    
    hierarchy = {
        "concept": concept,
        "super": super_concepts,
        "sub": sub_concepts,
        "level": lattice.get_level(concept)
    }
    
    return {"search_results": hierarchy}`
    },
    
    'lattice_jaccard_search': {
        title: 'lattice_jaccard_search - ìœ ì‚¬ë„ ê²€ìƒ‰',
        category: 'search',
        description: 'Jaccard ìœ ì‚¬ë„ë¡œ ìœ ì‚¬ ê°œë…ì„ ì°¾ìŠµë‹ˆë‹¤.',
        function: 'ì§‘í•© ìœ ì‚¬ë„ ê³„ì‚°',
        details: `â€¢ ì†ì„± ì§‘í•© ë¹„êµ
â€¢ Jaccard ê³„ìˆ˜ ê³„ì‚°: |Aâˆ©B| / |AâˆªB|
â€¢ ìœ ì‚¬ë„ ì„ê³„ê°’ ì ìš©
â€¢ ìƒìœ„ Nê°œ ìœ ì‚¬ ê°œë… ë°˜í™˜`,
        input: 'concept: str',
        output: 'similar_concepts: List',
        code: `def lattice_jaccard_search(state):
    concept = extract_concept(state["question"])
    lattice = load_lattice()
    
    similarities = []
    for other in lattice.concepts:
        if other != concept:
            jaccard = calculate_jaccard(
                concept.attributes,
                other.attributes
            )
            similarities.append((other, jaccard))
    
    # ìœ ì‚¬ë„ ìˆœ ì •ë ¬
    similarities.sort(key=lambda x: x[1], reverse=True)
    return {"search_results": similarities[:5]}`
    },
    
    'hybrid_search': {
        title: 'hybrid_search - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰',
        category: 'search',
        description: 'ëª¨ë“  ê²€ìƒ‰ ë°©ë²•ì„ í†µí•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.',
        function: 'ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ë³‘ë ¬ ì‹¤í–‰',
        details: `â€¢ Vector + Graph + Lattice ë™ì‹œ ì‹¤í–‰
â€¢ ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
â€¢ ê°€ì¤‘ì¹˜ ê¸°ë°˜ ì¬ìˆœìœ„í™”
â€¢ ë‹¤ì–‘ì„± ë³´ì¥`,
        input: 'question: str',
        output: 'integrated_results',
        code: `def hybrid_search(state):
    # ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰
    vector_results = vector_search(state)
    graph_results = graph_search(state)
    lattice_results = lattice_oc_search(state)
    
    # ê²°ê³¼ í†µí•©
    all_results = merge_results([
        (vector_results, 0.4),
        (graph_results, 0.3),
        (lattice_results, 0.3)
    ])
    
    # ì¬ìˆœìœ„í™”
    reranked = rerank_by_diversity(all_results)
    return {"search_results": reranked}`
    },
    
    'search_quality_eval': {
        title: 'search_quality_eval - ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€',
        category: 'quality',
        description: 'ê²€ìƒ‰ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.',
        function: 'ê´€ë ¨ì„±, ì»¤ë²„ë¦¬ì§€, ë‹¤ì–‘ì„± í‰ê°€',
        details: `â€¢ ê´€ë ¨ì„± ì ìˆ˜: ì§ˆë¬¸ê³¼ì˜ ë§¤ì¹­ë„
â€¢ ì»¤ë²„ë¦¬ì§€: ì •ë³´ì˜ ì™„ì „ì„±
â€¢ ë‹¤ì–‘ì„±: ë‹¤ì–‘í•œ ê´€ì  í¬í•¨ ì—¬ë¶€
â€¢ ì„ê³„ê°’: 0.7 ì´ìƒ í†µê³¼`,
        input: 'search_results',
        output: 'quality_score, decision',
        code: `def search_quality_eval(state):
    results = state["search_results"]
    
    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
    relevance = calculate_relevance(results, state["question"])
    coverage = calculate_coverage(results)
    diversity = calculate_diversity(results)
    
    # ì¢…í•© ì ìˆ˜
    quality_score = (relevance * 0.5 + 
                    coverage * 0.3 + 
                    diversity * 0.2)
    
    return {
        "search_quality": quality_score,
        "decision": "pass" if quality_score >= 0.7 else "fail"
    }`
    },
    
    'context_integration': {
        title: 'context_integration - ì»¨í…ìŠ¤íŠ¸ í†µí•©',
        category: 'generation',
        description: 'ê²€ìƒ‰ëœ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ì»¨í…ìŠ¤íŠ¸ë¡œ í†µí•©í•©ë‹ˆë‹¤.',
        function: 'ì¤‘ë³µ ì œê±° ë° ì •ë³´ í†µí•©',
        details: `â€¢ ì¤‘ë³µ ì •ë³´ ì œê±°
â€¢ ê´€ë ¨ì„± ìˆœ ì •ë ¬
â€¢ í† í° ìˆ˜ ì œí•œ ê³ ë ¤
â€¢ ë©”íƒ€ë°ì´í„° ë³´ì¡´`,
        input: 'search_results',
        output: 'integrated_context',
        code: `def context_integrate(state):
    results = state["search_results"]
    
    # ì¤‘ë³µ ì œê±°
    unique_chunks = remove_duplicates(results)
    
    # ê´€ë ¨ì„± ìˆœ ì •ë ¬
    sorted_chunks = sort_by_relevance(unique_chunks)
    
    # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    context = "\\n\\n".join([
        f"[{i+1}] {chunk.text}"
        for i, chunk in enumerate(sorted_chunks)
    ])
    
    return {"context": context}`
    },
    
    'prompt_generate': {
        title: 'prompt_generate - í”„ë¡¬í”„íŠ¸ ìƒì„±',
        category: 'generation',
        description: 'LLMì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.',
        function: 'êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì ìš©',
        details: `â€¢ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
â€¢ ì»¨í…ìŠ¤íŠ¸ ì£¼ì…
â€¢ ì§ˆë¬¸ í¬ë§·íŒ…
â€¢ Few-shot ì˜ˆì‹œ ì¶”ê°€ (ì„ íƒì )`,
        input: 'context, question',
        output: 'prompt: str',
        code: `def prompt_generate(state):
    template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
    
ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•˜ì„¸ìš”:
1. ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©
2. ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€
3. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ëª…ì‹œ

ë‹µë³€:"""
    
    prompt = template.format(
        context=state["context"],
        question=state["question"]
    )
    return {"prompt": prompt}`
    },
    
    'answer_generate': {
        title: 'answer_generate - ë‹µë³€ ìƒì„±',
        category: 'generation',
        description: 'LLMì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.',
        function: 'OpenAI GPT-4ë¡œ ë‹µë³€ ìƒì„±',
        details: `â€¢ GPT-4 ë˜ëŠ” GPT-3.5 ì‚¬ìš©
â€¢ Temperature ì„¤ì • (0.0-1.0)
â€¢ Max tokens ì œí•œ
â€¢ ìŠ¤íŠ¸ë¦¬ë° ì§€ì› (ì„ íƒì )`,
        input: 'prompt: str',
        output: 'answer: str',
        code: `def answer_generate(state):
    llm = ChatOpenAI(
        model="gpt-4",
        temperature=state.get("temperature", 0.0)
    )
    
    response = llm.invoke(state["prompt"])
    
    return {
        "answer": response.content,
        "model_used": "gpt-4",
        "tokens_used": response.usage
    }`
    },
    
    'answer_quality_eval': {
        title: 'answer_quality_eval - ë‹µë³€ í’ˆì§ˆ í‰ê°€',
        category: 'quality',
        description: 'ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.',
        function: 'ì™„ì „ì„±, ì •í™•ì„±, ì¼ê´€ì„± í‰ê°€',
        details: `â€¢ ì™„ì „ì„±: ì§ˆë¬¸ì˜ ëª¨ë“  ë¶€ë¶„ì— ë‹µë³€í–ˆëŠ”ê°€
â€¢ ì •í™•ì„±: ì»¨í…ìŠ¤íŠ¸ì™€ ì¼ì¹˜í•˜ëŠ”ê°€
â€¢ ì¼ê´€ì„±: ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ëœê°€
â€¢ ì„ê³„ê°’: 0.8 ì´ìƒ í†µê³¼`,
        input: 'answer, question, context',
        output: 'quality_score',
        code: `def answer_quality_eval(state):
    # LLMì„ ì‚¬ìš©í•œ í‰ê°€
    eval_prompt = f"""
ë‹µë³€ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.
ì§ˆë¬¸: {state["question"]}
ë‹µë³€: {state["answer"]}

í‰ê°€ ê¸°ì¤€:
- ì™„ì „ì„± (0-10)
- ì •í™•ì„± (0-10)
- ì¼ê´€ì„± (0-10)
"""
    
    eval_result = llm.invoke(eval_prompt)
    scores = parse_scores(eval_result)
    
    quality_score = sum(scores.values()) / 30
    return {"quality_score": quality_score}`
    },
    
    'hallucination_detect': {
        title: 'hallucination_detect - í™˜ê° ê°ì§€',
        category: 'quality',
        description: 'ë‹µë³€ì— í™˜ê°ì´ ìˆëŠ”ì§€ ê²€ì‚¬í•©ë‹ˆë‹¤.',
        function: 'ì‚¬ì‹¤ í™•ì¸ ë° ì¶œì²˜ ê²€ì¦',
        details: `â€¢ ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ ê°ì§€
â€¢ ì‚¬ì‹¤ ì™œê³¡ í™•ì¸
â€¢ ë…¼ë¦¬ì  ëª¨ìˆœ ê²€ì‚¬
â€¢ ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°`,
        input: 'answer, context',
        output: 'hallucination_score',
        code: `def hallucination_detect(state):
    # ë‹µë³€ì„ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    sentences = split_sentences(state["answer"])
    
    hallucinations = []
    for sentence in sentences:
        # ì»¨í…ìŠ¤íŠ¸ì—ì„œ ê·¼ê±° ì°¾ê¸°
        evidence = find_evidence(sentence, state["context"])
        if not evidence:
            hallucinations.append(sentence)
    
    hallucination_ratio = len(hallucinations) / len(sentences)
    
    return {
        "hallucination_score": hallucination_ratio,
        "flagged_sentences": hallucinations,
        "decision": "pass" if hallucination_ratio < 0.2 else "rewrite"
    }`
    },
    
    'query_rewrite': {
        title: 'query_rewrite - ì¿¼ë¦¬ ì¬ì‘ì„±',
        category: 'generation',
        description: 'ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•©ë‹ˆë‹¤.',
        function: 'ì§ˆë¬¸ ê°œì„  ë° ëª…í™•í™”',
        details: `â€¢ ëª¨í˜¸í•œ í‘œí˜„ ì œê±°
â€¢ í‚¤ì›Œë“œ í™•ì¥
â€¢ ë™ì˜ì–´ ì¶”ê°€
â€¢ ê²€ìƒ‰ ì¹œí™”ì  í˜•íƒœë¡œ ë³€í™˜`,
        input: 'original_question, failure_reason',
        output: 'rewritten_question',
        code: `def query_rewrite(state):
    rewrite_prompt = f"""
ì›ë˜ ì§ˆë¬¸: {state["original_question"]}
ì‹¤íŒ¨ ì´ìœ : {state.get("failure_reason", "ë‚®ì€ í’ˆì§ˆ")}

ì´ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  ê²€ìƒ‰í•˜ê¸° ì‰½ê²Œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì‚¬ìš©
- ëª¨í˜¸í•œ í‘œí˜„ ì œê±°
- í•µì‹¬ ì˜ë„ ìœ ì§€

ì¬ì‘ì„±ëœ ì§ˆë¬¸:"""
    
    response = llm.invoke(rewrite_prompt)
    
    return {
        "question": response.content,
        "rewrite_count": state.get("rewrite_count", 0) + 1
    }`
    },
    
    'answer_post': {
        title: 'answer_post - ë‹µë³€ í›„ì²˜ë¦¬',
        category: 'generation',
        description: 'ìµœì¢… ë‹µë³€ì„ í¬ë§·íŒ…í•˜ê³  ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.',
        function: 'ë‹µë³€ ì •ë¦¬ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€',
        details: `â€¢ í¬ë§·íŒ… ì ìš©
â€¢ ì°¸ì¡° ì¶”ê°€
â€¢ ì‹ ë¢°ë„ ì ìˆ˜ í‘œì‹œ
â€¢ ë¡œê¹… ë° ë¶„ì„ìš© ë°ì´í„° ì €ì¥`,
        input: 'answer, metadata',
        output: 'final_answer',
        code: `def answer_post(state):
    # í¬ë§·íŒ…
    formatted_answer = format_answer(state["answer"])
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    final_answer = f"""
{formatted_answer}

---
ê²€ìƒ‰ ë°©ë²•: {state["search_method"]}
ì‹ ë¢°ë„: {state.get("quality_score", 0):.2f}
ì²˜ë¦¬ ì‹œê°„: {state.get("processing_time", 0):.2f}ì´ˆ
"""
    
    # ë¡œê¹…
    log_interaction(state)
    
    return {"final_answer": final_answer}`
    },
    
    'end': {
        title: 'END - ì¢…ë£Œ ë…¸ë“œ',
        category: 'system',
        description: 'ì›Œí¬í”Œë¡œìš°ë¥¼ ì¢…ë£Œí•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.',
        function: 'ìµœì¢… ìƒíƒœ ì •ë¦¬ ë° ë°˜í™˜',
        details: `â€¢ ìµœì¢… ë‹µë³€ ë°˜í™˜
â€¢ ë¦¬ì†ŒìŠ¤ ì •ë¦¬
â€¢ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
â€¢ ì„¸ì…˜ ì¢…ë£Œ`,
        input: 'final_answer',
        output: 'ì‚¬ìš©ìì—ê²Œ ë‹µë³€ ì „ë‹¬',
        code: `def end_workflow(state):
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
    metrics = {
        "total_time": time.time() - state["start_time"],
        "search_method": state["search_method"],
        "quality_score": state.get("quality_score", 0),
        "rewrite_count": state.get("rewrite_count", 0)
    }
    
    save_metrics(metrics)
    
    return {
        "final_answer": state["final_answer"],
        "metrics": metrics
    }`
    }
};

function showNodeDetails(event, d) {
    const modal = document.getElementById('node-modal');
    const modalTitle = document.getElementById('modal-title');
    const modalBody = document.getElementById('modal-body');
    
    const details = nodeDetails[d.id] || {
        title: d.label,
        category: d.type,
        description: `${d.type} íƒ€ì…ì˜ ë…¸ë“œì…ë‹ˆë‹¤.`,
        function: 'ê¸°ëŠ¥ ì„¤ëª… ì—†ìŒ',
        details: 'ìƒì„¸ ì •ë³´ ì—†ìŒ',
        input: 'N/A',
        output: 'N/A',
        code: ''
    };
    
    // ì¹´í…Œê³ ë¦¬ë³„ ìƒ‰ìƒ
    const categoryColors = {
        'system': '#10b981',
        'indexing': '#3b82f6',
        'router': '#8b5cf6',
        'search': '#f59e0b',
        'quality': '#ec4899',
        'generation': '#06b6d4'
    };
    
    const categoryColor = categoryColors[details.category] || '#6b7280';
    
    modalTitle.textContent = details.title;
    modalBody.innerHTML = `
        <div style="background: ${categoryColor}; color: white; padding: 10px; border-radius: 5px; margin-bottom: 15px;">
            <strong>ì¹´í…Œê³ ë¦¬:</strong> ${details.category.toUpperCase()}
        </div>
        
        <div style="margin-bottom: 20px;">
            <h4 style="color: #1f2937; margin-bottom: 10px;">ğŸ“‹ ì„¤ëª…</h4>
            <p style="color: #4b5563; line-height: 1.6;">${details.description}</p>
        </div>
        
        <div style="margin-bottom: 20px;">
            <h4 style="color: #1f2937; margin-bottom: 10px;">âš™ï¸ ì£¼ìš” ê¸°ëŠ¥</h4>
            <p style="color: #4b5563; font-weight: 600;">${details.function}</p>
            <div style="background: #f3f4f6; padding: 10px; border-radius: 5px; margin-top: 10px;">
                <pre style="margin: 0; color: #374151; white-space: pre-wrap;">${details.details}</pre>
            </div>
        </div>
        
        <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 15px; margin-bottom: 20px;">
            <div style="background: #e0f2fe; padding: 15px; border-radius: 5px;">
                <h5 style="color: #0369a1; margin-bottom: 5px;">ğŸ“¥ ì…ë ¥</h5>
                <p style="color: #0c4a6e; font-size: 14px; margin: 0;">${details.input}</p>
            </div>
            <div style="background: #dcfce7; padding: 15px; border-radius: 5px;">
                <h5 style="color: #166534; margin-bottom: 5px;">ğŸ“¤ ì¶œë ¥</h5>
                <p style="color: #14532d; font-size: 14px; margin: 0;">${details.output}</p>
            </div>
        </div>
        
        ${details.code ? `
        <div style="margin-bottom: 20px;">
            <h4 style="color: #1f2937; margin-bottom: 10px;">ğŸ’» ì½”ë“œ ì˜ˆì‹œ</h4>
            <div style="background: #1f2937; color: #e5e7eb; padding: 15px; border-radius: 5px; overflow-x: auto;">
                <pre style="margin: 0; font-family: 'Courier New', monospace; font-size: 13px;"><code>${details.code}</code></pre>
            </div>
        </div>
        ` : ''}
    `;
    
    modal.style.display = 'block';
}

// Initialize on page load
document.addEventListener('DOMContentLoaded', () => {
    initializeWorkflow();
    
    // Setup modal close
    const modal = document.getElementById('node-modal');
    const span = document.getElementsByClassName('close')[0];
    
    span.onclick = () => modal.style.display = 'none';
    window.onclick = (event) => {
        if (event.target === modal) modal.style.display = 'none';
    };
});